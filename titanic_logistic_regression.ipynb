{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3143e28a",
   "metadata": {},
   "source": [
    "# Análisis y Modelo de Regresión Logística — Titanic\n",
    "\n",
    "Este notebook realiza un análisis paso a paso y entrena un **modelo de regresión logística** para predecir la supervivencia en el dataset del Titanic.\n",
    "\n",
    "Se incluyen: descripción de valores faltantes, estrategias de imputación, codificación de variables categóricas, entrenamiento del modelo y evaluación (accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d102fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaciones\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "print(\"Librerías importadas correctamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4af2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datasets (asegúrate que train.csv y test.csv estén en el mismo directorio)\n",
    "train = pd.read_csv('/mnt/data/train.csv')\n",
    "test = pd.read_csv('/mnt/data/test.csv')\n",
    "\n",
    "print(\"Train shape:\", train.shape)\n",
    "print(\"Test shape:\", test.shape)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fed04f",
   "metadata": {},
   "source": [
    "## a) Relevamiento de valores faltantes\n",
    "\n",
    "A continuación se muestra la cantidad de valores faltantes por variable para `train` y `test`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b89b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteo de valores faltantes\n",
    "missing_train = train.isnull().sum().sort_values(ascending=False)\n",
    "missing_test = test.isnull().sum().sort_values(ascending=False)\n",
    "\n",
    "print(\"Valores faltantes - train:\\n\", missing_train[missing_train>0])\n",
    "print(\"\\nValores faltantes - test:\\n\", missing_test[missing_test>0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d07dbd",
   "metadata": {},
   "source": [
    "## b) Estrategia para completar valores faltantes\n",
    "\n",
    "- **Age**: imputaremos con la mediana (agrupando por `Sex` y `Pclass` para ser más precisos).\n",
    "- **Fare**: imputación por la mediana (solo aparece 1 faltante en test).\n",
    "- **Cabin**: extraeremos la letra del camarote (Deck). Para los valores faltantes usaremos un valor 'M' (Missing).\n",
    "- **Embarked**: imputar con la moda (valor más frecuente).\n",
    "\n",
    "Usaremos un `ColumnTransformer` y un `Pipeline` para encadenar imputación, codificación y escalado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ee3c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering: extraer 'Deck' desde 'Cabin' y 'Title' desde 'Name'\n",
    "def extract_deck(cabin):\n",
    "    if pd.isna(cabin):\n",
    "        return 'M'  # Missing\n",
    "    return str(cabin)[0]\n",
    "\n",
    "def extract_title(name):\n",
    "    # extrae título desde el nombre (Mr., Mrs., Miss., etc.)\n",
    "    if pd.isna(name):\n",
    "        return 'Unknown'\n",
    "    parts = name.split(',')\n",
    "    if len(parts) > 1:\n",
    "        title_section = parts[1]\n",
    "        title = title_section.split('.')[0].strip()\n",
    "        return title\n",
    "    return 'Unknown'\n",
    "\n",
    "train['Deck'] = train['Cabin'].apply(extract_deck)\n",
    "test['Deck'] = test['Cabin'].apply(extract_deck)\n",
    "\n",
    "train['Title'] = train['Name'].apply(extract_title)\n",
    "test['Title'] = test['Name'].apply(extract_title)\n",
    "\n",
    "# Mostrar algunas filas\n",
    "train[['Name','Title','Cabin','Deck']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0335494",
   "metadata": {},
   "source": [
    "### Preparación de variables para el modelo\n",
    "Seleccionaremos estas features: `Pclass`, `Sex`, `Age`, `SibSp`, `Parch`, `Fare`, `Embarked`, `Deck`, `Title`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd347085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables a usar\n",
    "features = ['Pclass','Sex','Age','SibSp','Parch','Fare','Embarked','Deck','Title']\n",
    "target = 'Survived'\n",
    "\n",
    "X = train[features].copy()\n",
    "y = train[target].copy()\n",
    "X_test = test[features].copy()\n",
    "\n",
    "X.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb049865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformaciones por tipo de variable\n",
    "numeric_features = ['Age','SibSp','Parch','Fare']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_features = ['Pclass','Sex','Embarked','Deck','Title']\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b042a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline completo con regresión logística\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# Dividir en entrenamiento y validación\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Entrenar\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar en conjunto de validación\n",
    "y_pred = model.predict(X_val)\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f'Accuracy en validación: {accuracy:.4f}')\n",
    "\n",
    "# Matriz de confusión y reporte\n",
    "print('\\nMatriz de confusión:')\n",
    "print(confusion_matrix(y_val, y_pred))\n",
    "print('\\nReporte de clasificación:')\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cd3d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluación adicional con cross-validation (5 folds)\n",
    "cv_scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
    "print('Cross-val accuracy (5 folds):', cv_scores)\n",
    "print('Media CV accuracy: {:.4f} (+/- {:.4f})'.format(cv_scores.mean(), cv_scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cd7776",
   "metadata": {},
   "source": [
    "#### Nota sobre la imputación de `Age`\n",
    "Además de la imputación por mediana global usada en el pipeline, una alternativa es imputar `Age` usando la mediana por `Sex` y `Pclass`. A continuación se muestra cómo hacerlo y volver a entrenar si se desea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b575ee22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputación avanzada de Age por Sex + Pclass (opcional)\n",
    "X_adv = X.copy()\n",
    "X_adv['Age'] = X_adv.groupby(['Sex','Pclass'])['Age'].apply(lambda grp: grp.fillna(grp.median()))\n",
    "\n",
    "# Si aún quedan NaN (grupos sin datos), llenar con mediana global\n",
    "X_adv['Age'] = X_adv['Age'].fillna(X_adv['Age'].median())\n",
    "\n",
    "# Re-entrenar un modelo rápido para comparar\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "model_adv = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),  # preprocessor rellenará numicamente Age si queda NaN\n",
    "    ('classifier', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "scores_adv = cross_val_score(model_adv, X_adv, y, cv=5, scoring='accuracy')\n",
    "print('CV accuracy con imputación por grupo (5 folds):', scores_adv)\n",
    "print('Media:', scores_adv.mean(), 'Std:', scores_adv.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbc8da0",
   "metadata": {},
   "source": [
    "## Predicción sobre el set `test` y guardado de archivo de salida\n",
    "El dataset `test.csv` no contiene la columna `Survived`. Por eso medimos accuracy usando una partición del `train`. De todas formas generaremos predicciones sobre `test` y las guardaremos en `submission.csv`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b37c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reentrenar el modelo con todo el set de train\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predecir sobre test\n",
    "test_preds = model.predict(X_test)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'PassengerId': test['PassengerId'],\n",
    "    'Survived': test_preds\n",
    "})\n",
    "\n",
    "output_path = '/mnt/data/submission_titanic_logreg.csv'\n",
    "submission.to_csv(output_path, index=False)\n",
    "print('Archivo de predicción guardado en:', output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a14e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar modelo entrenado para uso posterior\n",
    "model_path = '/mnt/data/titanic_logreg_pipeline.joblib'\n",
    "joblib.dump(model, model_path)\n",
    "print('Pipeline guardado en:', model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef032f89",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "- Se completaron los valores faltantes con estrategias razonables (medianas y modas).\n",
    "- Se entrenó un primer modelo de regresión logística.\n",
    "- Se reportó el accuracy en un conjunto de validación y con validación cruzada.\n",
    "\n",
    "Si querés, puedo:\n",
    "- Probar otras técnicas de imputación (KNNImputer, tasa de missing indicator, etc.).\n",
    "- Probar otros modelos (Random Forest, XGBoost) y comparar métricas.\n",
    "- Afinar hiperparámetros con GridSearchCV o RandomizedSearchCV.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
